---
title: "Machine Learning"
subtitle: "Homework 4"
author: "Varada Shrotri"
date: "10 February 2021"
output: 
    pdf_document:
        number_sections: true    
---

```{r library setup, include=FALSE, warning = F, echo=FALSE, results = FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```

# Load relevant libraries 

```{r message=F, warning=FALSE}
library(dplyr)
library(caret)
library(glmnet)
```


# Load the data

examine the structure of csv data file, and fix the data types incorrectly identified by R when importing from CSV.

```{r warning=FALSE}
setwd("D:\\Documents\\Harris\\Padhai\\5. Quarter 5\\Booth ML\\Homework\\Homework 4")

# Clear memory
rm(list=ls())

STCdata_A<-read.csv('travelData.csv')
STCdata_A<-STCdata_A[,-1]
```


```{r}
str(STCdata_A)
```

Notice that some columns are identified as numerical or integer, but really they should be factors.

For instance, we have that column `From.Grade`
```{r}
n_distinct(STCdata_A$From.Grade, na.rm = FALSE)   ## n_distinct is a function from dplyr package
```
only has 11 levels. It might be a better idea to treat it as a factor instead.

Fixing incorrectly classified data types as follows:
```{r}
STCdata_A <- mutate_at(STCdata_A, vars(From.Grade), as.factor)

```

Check that indeed the column represents a factor:
```{r}
str( STCdata_A$From.Grade )
```

Fix other columns that are numeric at the moment, but could be converted to factors.
The following line first finds numeric columns and then identifies the number of unique elements in each one.
```{r}
( unique.per.column <- sapply( dplyr::select_if(STCdata_A, is.numeric), n_distinct ) )
```

Let us convert every column that has less than 15 unique values into a factor.
The following line identify names of such columns.
```{r}
( column.names.to.factor <- names(unique.per.column)[unique.per.column < 15] )
```

From this, we can see that the columns 
`To.Grade`, `Is.Non.Annual.`, `Days`,
`CRM.Segment`, `Parent.Meeting.Flag`, `MDR.High.Grade`, 
`School.Sponsor`, `NumberOfMeetingswithParents`, `SingleGradeTripFlag`
can be converted to factors.
We can also convert the output `Retained.in.2012.`

Convert these columns into factors.

```{r}
STCdata_A <- mutate_at(STCdata_A, column.names.to.factor, as.factor)
```

Wrok on date columns.
```{r}
date.columns = c('Departure.Date', 'Return.Date', 'Deposit.Date', 'Early.RPL', 'Latest.RPL', 'Initial.System.Date', 'FirstMeeting', 'LastMeeting')

STCdata_A <- mutate_at(STCdata_A, date.columns, function(x) as.Date(x, format = "%m/%d/%Y"))

```

And finally I change all the character columns to factors as well.

```{r}
STCdata_A <- mutate_if(STCdata_A, is.character, as.factor)
```

Finally:
```{r}
str(STCdata_A)
```


# Data preprocessing

The data contains a number of columns with missing values.

The following tells the number of missing values in each column.
```{r}
sapply(STCdata_A, function(x) sum(is.na(x)))
```

The function `fixNAs` below fixes missing values. 
The function defines reactions:

 - adds a new category "FIXED_NA" for a missing value of a categorical/factor variable;
 - fills zero value for a missing value of a numeric variable;
 - fills "1900-01-01" for a missing value of a date variable.

Then it loops through all columns in the dataframe, 
reads their types, and loops through all the values, 
applying the defined reaction to any missing data point. 
In addition, the function creates a surrogate dummy 
variable for each column containing at least one missing value
(for example, `Special.Pay_surrogate`), which takes a value 
of 1 whenever the original variable (`Special.Pay`) has a
missing value, and 0 otherwise.

```{r}
# Create a custom function to fix missing values ("NAs") and 
# preserve the NA info as surrogate variables
fixNAs <- function(data_frame){
  # Define reactions to NAs
  integer_reac <- 0
  factor_reac <- "FIXED_NA"
  character_reac <- "FIXED_NA"
  date_reac <- as.Date("1900-01-01")
  
  # Loop through columns in the data frame 
  # and depending on which class the
  # variable is, apply the defined reaction and 
  # create a surrogate
  
  for (i in 1:ncol(data_frame)) {
    if (class(data_frame[,i]) %in% c("numeric","integer")) {
      if (any(is.na(data_frame[,i]))) {
        data_frame[,paste0(colnames(data_frame)[i],"_surrogate")] <-
          as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
        data_frame[is.na(data_frame[,i]), i] <- integer_reac
      }
    } else
      if (class(data_frame[,i]) %in% c("factor")) {
        if (any(is.na(data_frame[,i]))){
          data_frame[,i]<-as.character(data_frame[,i])
          data_frame[,paste0(colnames(data_frame)[i],"_surrogate")] <-
            as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
          data_frame[is.na(data_frame[,i]),i]<-factor_reac
          data_frame[,i]<-as.factor(data_frame[,i])
        }
      } else {
        if (class(data_frame[,i]) %in% c("character")) {
          if (any(is.na(data_frame[,i]))){
            data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
              as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
            data_frame[is.na(data_frame[,i]),i]<-character_reac
          }
        } else {
          if (class(data_frame[,i]) %in% c("Date")) {
            if (any(is.na(data_frame[,i]))){
              data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
                as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
              data_frame[is.na(data_frame[,i]),i]<-date_reac
            }
          }
        }
      }
  }
  
  return(data_frame)
}
```

Apply the above defined function to the data frame.

```{r}
STCdata_A<-fixNAs(STCdata_A)
```

columns do not have any missing values any more.
```{r}
any( sapply(STCdata_A, function(x) sum(is.na(x))) > 0)
```


Combine the rare categories. Levels that do not occur often 
during training tend not to have reliable effect estimates 
and contribute to over-fit. 

Check for rare categories in the variable `Group.State`.
```{r}
table(STCdata_A$Group.State)
```


Create a custom function to combine rare categories.
The function again loops through all the columns in the dataframe,
reads their types, and creates a table of counts 
for each level of the factor/categorical variables. All
levels with counts less than the `mincount` are combined into "other."
The function combines rare categories into "Other."+the name of the 
original variable (for example, `Other.State`).
This function has two arguments: 

- the name of the dataframe; and 
- the count of observations in a category to define "rare."


```{r}
combinerarecategories<-function(data_frame,mincount){
  for (i in 1:ncol(data_frame)) {
    a<-data_frame[,i]  #ith column of the df
    replace <- names(which(table(a) < mincount)) 
    levels(a)[levels(a) %in% replace] <-
      paste("Other", colnames(data_frame)[i], sep=".")
    data_frame[,i]<-a 
  }
  return(data_frame) 
}
```

Combine categories with $<10$ values in `STCdata` into "Other."

```{r}
STCdata_A<-combinerarecategories(STCdata_A,10) 
```

```{r}
table(STCdata_A$Group.State)
```



# Split the data into training and testing sets

- a random seed ensures that the random-number generator 
  is initialized identically in each run; and 
- the `inTrain` vector is created once and can then be applied 
  anytime the data needs to be split. 
  
By default, the code sets 500 data points in the test set, 
and the remainder 1,889 into the training set.


```{r}
# set a random number generation seed to 
# ensure that the split is the same every time
set.seed(233) 

inTrain <- createDataPartition(
  y = STCdata_A$Retained.in.2012.,
  p = 1888/2389, 
  list = FALSE)
df.train <- STCdata_A[ inTrain,]
df.test <- STCdata_A[ -inTrain, ]
```

Check that both the training and test sets have a similar
proportion of positive and negative cases.

```{r}
print('Training set proportion:')
table(df.train$Retained.in.2012.) / nrow(df.train)
print('Test set proportion:')
table(df.test$Retained.in.2012.) / nrow(df.test)
```

# Fitting a logistic regression model

Fit a logistic regression model with all the variables included on the training set.

```{r}
lgfit.all <- glm(Retained.in.2012.~ ., 
                 data=df.train, 
                 family="binomial")
summary(lgfit.all)
```

The model is overfit. It has too many insignificant variables.

Fit a much simpler model using stepwise regressions. 


```{r}
# Start from a null model with intercept only, and add one covarite at a time until #maximum BIC.
lgfit.null <- glm(Retained.in.2012.~ 1, 
                 data=df.train, family="binomial")

lgfit.selected <- step(lgfit.null,                  # the starting model for our search
                       scope=formula(lgfit.all),    # the largest possible model that we will consider.
                       direction="forward", 
                       k=log(nrow(df.train)),       # by default step() uses AIC, but by
                                                    # multiplying log(n) on the penalty, we get BIC.
                                                    # See ?step -> Arguments -> k
                       trace=1)
```

The algorithm stops once none of the 1-step expanded models lead to a lower BIC.

This is the selected model.
```{r}
summary(lgfit.selected)
```

Oredicting probabilities
```{r}
phat.list = list() #Store the test phat for different methods here 

phat.lgfit.selected <- predict(lgfit.selected, 
                               newdata = df.test,
                               type = "response")

phat.list$logit <- matrix(phat.lgfit.selected, ncol=1)
```


Using Lasso for variable selection

First, create a model matrix that will be used as an input to the package.
```{r}
X <- model.matrix(formula(lgfit.all), STCdata_A)
#need to subtract the intercept
X <- X[,-1]

X.train = X[ inTrain, ]
X.test = X[ -inTrain, ]
```

Next, run 5-fold cross-validation.
```{r}
cv.l1.lgfit <- cv.glmnet(
  x       = X.train, 
  y       = df.train$Retained.in.2012.,
  family  = "binomial", 
  alpha   = 1,   #alpha=0 gives ridge regression
  nfolds  = 5)
```

Plot the cross-validation curve, which shows an estimate of out-of-sample deviances a function of the tuning parameter $\lambda$.

```{r}
plot(cv.l1.lgfit, sign.lambda=-1)
```

Plot the fitted coefficients as a function of $\lambda$.
Note that `cv.l1.lgfit$glmnet.fit` corresponds to a fitted glmnet object for the full data.

```{r}
glmnet.fit <- cv.l1.lgfit$glmnet.fit
# What does glmnet.fit do<

plot(glmnet.fit, xvar = "lambda")
abline(v = log(cv.l1.lgfit$lambda.min), lty=2, col="red")
abline(v = log(cv.l1.lgfit$lambda.1se), lty=2, col="green")
legend("topright", legend=c("min", "1se"), lty=2, col=c("red", "green"))
```

For the predictive model, use 1 standard error $\lambda$.
variables that are selected by the lasso:

```{r}
betas <- coef(cv.l1.lgfit, s = "lambda.1se")
model.1se <- which(betas[2:length(betas)]!=0)
colnames(X[,model.1se])
```

use model to predict probabilities on the test set.

```{r}
phat.l1.lgfit <- predict(glmnet.fit,
                         newx = X.test,
                         s = cv.l1.lgfit$lambda.1se,
                         type = "response")
phat.list$lasso <- matrix(phat.l1.lgfit, ncol = 1)

```


# Questions

## How well does logistic regression do?

1. Create a confusion matrix for two logistic regression models build above.
   Use probabilities `phat.lgfit.selected` and `phat.l1.lgfit` to do so.
   
   To solve this question, you need to make a major decision. 
   What should the cutoff or "threshold" for the probability be,
   above which you will label a customer as being classified as "retained?"
   In our case, the data is slightly unbalanced---about 60.72% of data points are in Class 1.
   For very unbalanced data, we would first need to balance it (over- or under-sample).
   In this case, the benefits of balancing are unclear, hence one can implement 
   the average probability of being retained as a cutoff. 
   
   Predict classification using 0.6072 threshold.
   
   What can we see from the confusion matrices?
   
```{r warning=FALSE}
# Confusion Matrix Function from Tabloid Example
getConfusionMatrix = function(y,phat,thr=0.6072) {
   yhat = as.factor( ifelse(phat > thr, 1, 0) )
   #print(yhat)
   confusionMatrix(yhat, as.factor(y))
}

# Misclassification Rate Function from Tabloid Example
loss.misclassification.rate = function(y, phat, thr=0.5) 
   1 - getConfusionMatrix(y, phat, thr)$overall[1]

cfm1 <- getConfusionMatrix(df.test$Retained.in.2012., phat.list$logit[,1], 0.6072)
cfm2 <- getConfusionMatrix(df.test$Retained.in.2012., phat.list$lasso[,1], 0.6072)

print(cfm1, printStates = F)
print(cfm2, printStates = F)

```
   
   
2. Plot ROC curves for the two classifiers and report the area under the curve.

   Note that the AUC of an error-free classifier would be 100%, 
   and an AUC of a random guess would be 50%. For values in-between,
   we can think of AUC as follows:
   
   - 90%+ = excellent,
   - 80–90% = very good, 
   - 70–80% = good, 
   - 60–70% = so-so, and
   - below 60% = not much value.
   
```{r warning=FALSE}
library(ROCR)
```

```{r fig.width=6, fig.height=6}

lossf = function(y,phat,wht=0.0000001) {
   if(is.factor(y)) y = as.numeric(y)-1
   phat = (1-wht)*phat + wht*.5
   py = ifelse(y==1, phat, 1-phat)
   return(-2*sum(log(py)))
}

nmethod = length(phat.list)
phatBest = matrix(0.0,nrow(df.test),nmethod) #pick off best from each method


colnames(phatBest) = names(phat.list)
for(i in 1:nmethod) {
   nrun = ncol(phat.list[[i]])
   lvec = rep(0,nrun)
   for(j in 1:nrun) lvec[j] = lossf(df.test$Retained.in.2012.,phat.list[[i]][,j])
   imin = which.min(lvec)
   phatBest[,i] = phat.list[[i]][,imin]
}


for(i in 1:ncol(phatBest)) {
   pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
   perf = performance(pred, measure = "tpr", x.measure = "fpr")
   
   if (i == 1) {
     plot(perf, col = 1, lwd = 2,
          main= 'ROC curve', xlab='FPR', ylab='TPR', cex.lab=1)
   } else {
     plot(perf, add = T, col = i, lwd = 2)
   }
}
abline(0,1,lty=2)
legend("topleft",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod))

for(i in 1:ncol(phatBest)) {
  pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
  perf <- performance(pred, measure = "auc")
  print(paste0("AUC ", names(phat.list)[i], " :: ", perf@y.values[[1]]))
}

```


3. Plot lift curves for the two classifiers.


```{r warning=FALSE}
pred = prediction(phatBest[,1], df.test$Retained.in.2012.)
perf = performance(pred, measure = "lift", x.measure = "rpp", lwd=2)
plot(perf, col=1, ylim=c(0,2))
abline(h=1, lty=2)

for(i in 2:ncol(phatBest)) {
   pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
   perf = performance(pred, measure = "lift", x.measure = "rpp")
   plot(perf, add = T, col = i, lwd = 2)
}
legend("topright",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod), lwd=2)


```

4. Create the profit curve (the amount of net profit vs the number 
   of groups targeted for promotion) for the two classifiers. 
   Suppose that the benefit of retaining a group is $100, 
   while the cost of a promotion is $40. 
   
   How many groups should be targeted to maximize the profit?
   
   How would this number change as the ratio between the benefit and cost changes?

   You can refer to the following code that plots a profit curve:
   
```{r}
# Function to plot a profit curve
#
# Inputs:
#  - benefitTP(FN/FP/TN): the net benefit for a true positive (false negative,...)
#      which is positive for a gain, and negative for a loss
#  - y: vector of true labels, which has to be labeled as "0" and "1"
#  - phat: vector of predicted probabilities
# Outputs:
#    the function returns the profit curve

ProfitCurve <- function(benefitTP, benefitFN, benefitFP, benefitTN, y, phat, col){
  
  if(length(y) != length(phat)) stop("Length of y and phat not identical")
  if(length(levels(y))!=2 | levels(y)[1]!="0" | levels(y)[2]!="1")
    stop("y should be a vector of factors, only with levels 0 and 1")
  
  n <- length(y)
  df <- data.frame(y, phat)
  # Order phat so that we can pick the k highest groups for promotion
  df <- df[order(df[,2], decreasing = T),]
  TP <- 0; FP <- 0; FN <- table(y)[2]; TN <- table(y)[1]
  
  # Initializing the x and y coordinates of the plot
  ratio.vec <- seq(0,n)/n
  profit.vec <- rep(0,n+1)
  profit.vec[1] <- FN * benefitFN + TN * benefitTN
  
  for(k in 1:n){ # k is the number of groups classified as "YES"
    # In every round, we are picking one more group for promotion.
    # If this group was ratained (positive), then in this round, it is classified
    # as a "YES" instead of "NO" before. The confusion matrix is updated each round
    # with one more TP, and one less FN. It's similar when the group was not ratained.
    if(df[k,1]=="1"){TP <- TP + 1; FN <- FN - 1}
    else{FP <- FP + 1; TN <- TN - 1}
    #print(paste(TP, FP, TP-FP, benefitTP, benefitFP))
    profit.vec[k+1] <- TP*benefitTP + FP*benefitFP + FN*benefitFN + TN*benefitTN
  }
  
  plt <- plot(ratio.vec, profit.vec, type="l", lwd=2, col=4, main="Profit Curve",
              xlab="Percentage of Targetted Groups", ylab="Profit")
  abline(b=(profit.vec[n+1]-profit.vec[1]), a=profit.vec[1], lty=2) #Random guess
  return(plt)
}

logit.pf.curve <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,1])

lasso.pf.curve <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,2])


```


5. Develop a decision tree, random forest, and a boosting model using the training data.
   
   Report ROC, AUC, lift, and profit curves for these models.
   
   How do these methods compare to the logistic regression models?

```{r}
### Random Forest

p=ncol(STCdata_A)-1

hyper_grid_rf <- expand.grid(
  mtry       = c(p, ceiling(sqrt(p))),
  node_size  = c(5, 10, 20)
)

# we will store phat values here
phat.list$rf = matrix(0.0, nrow(df.test), nrow(hyper_grid_rf))  

for(i in 1:nrow(hyper_grid_rf)) {
  # train model
  rf.model <- ranger(
    formula         = Retained.in.2012.~.,
    data            = STCdata_A, 
    num.trees       = 250,
    mtry            = hyper_grid_rf$mtry[i],
    min.node.size   = hyper_grid_rf$node_size[i],
    probability     = TRUE, 
    seed            = 99
  )   
   
   # predict for random forest returns
   # a matrix of class probabilities 
   #    one column for each class and one row for each input
   # we want to record probability for class=1, 
   # which is the second column of the output
   phat = predict(rf.model, data=df.test)$predictions[,2]
   phat.list$rf[,i]=phat
}


# ROC, AUC, lift, and profit curves


```   


```{r}

X.train_1 = Matrix::sparse.model.matrix(Retained.in.2012. ~., data = df.train)[,-1]
y.train_1 = as.numeric(df.train$Retained.in.2012.)-1

X.test_1 = Matrix::sparse.model.matrix(Retained.in.2012. ~., data = df.test)[,-1]
y.test_1 = as.numeric(df.test$Retained.in.2012.)-1


hyper_grid_xgb <- expand.grid(
  shrinkage = c(.01, .1),         ## controls the learning rate
  interaction.depth = c(1, 2, 4), ## tree depth
  #bag.fraction = c(.5, .65, .8),  ##percent of training data to sample for each tree
  #optimal_trees = 0,              # a place to dump results
  #min_RMSE = 0,                    # a place to dump results
  nrounds = c(1000, 2000) 
)

# we will store phat values here
phat.list$boost = matrix(0.0,nrow(df.test),nrow(hyper_grid_xgb))


for(i in 1:nrow(hyper_grid_xgb)) {
  # create parameter list
  params <- list(
    eta = hyper_grid_xgb$shrinkage[i],
    max_depth = hyper_grid_xgb$interaction.depth[i]
  )
   
  # reproducibility
  set.seed(4776)
  
  # train model
  xgb.model <- xgboost(
    data      = X.train_1,
    label     = y.train_1,
    params    = params,
    nrounds   = hyper_grid_xgb$nrounds[i],
    objective = "binary:logistic",     # for regression models
    verbose   = 0,                     # silent
    verbosity = 0                      # silent
  )
   
  phat = predict(xgb.model, newdata=X.test_1)
  phat.list$boost[,i] = phat
}

```


```{r}
# Decision Tree
library(rpart)
big_tree = rpart(Retained.in.2012.~., data=df.train, 
                 control=rpart.control(minsplit=10,  
                                       cp=0.0001,    
                                       xval=10)      
                 )

plotcp(big_tree)

```

```{r}

cptable = printcp(big_tree)

index_cp_min = which.min(cptable[,"xerror"])
(cp_min = cptable[ index_cp_min, "CP" ])  

library(rpart.plot)
optimal.tree = prune(big_tree, cp=cp_min)
rpart.plot(optimal.tree)

yhat = predict(optimal.tree, df.test)

phat.list$dec_tree <- matrix(yhat, ncol = 1)

mean(yhat == df.test$Retained.in.2012.)

tb_tree = table(predictions = yhat, 
                actual = df.test$Retained.in.2012.)  
print(tb_tree)

```

```{r}
nmethod = length(phat.list)
phatBest = matrix(0.0,nrow(df.test),nmethod) #pick off best from each method


colnames(phatBest) = names(phat.list)
for(i in 1:nmethod) {
   nrun = ncol(phat.list[[i]])
   lvec = rep(0,nrun)
   for(j in 1:nrun) lvec[j] = lossf(df.test$Retained.in.2012.,phat.list[[i]][,j])
   imin = which.min(lvec)
   phatBest[,i] = phat.list[[i]][,imin]
}


for(i in 1:ncol(phatBest)) {
   pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
   perf = performance(pred, measure = "tpr", x.measure = "fpr")
   
   if (i == 1) {
     plot(perf, col = 1, lwd = 2,
          main= 'ROC curve', xlab='FPR', ylab='TPR', cex.lab=1)
   } else {
     plot(perf, add = T, col = i, lwd = 2)
   }
}
abline(0,1,lty=2)
legend("topleft",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod))

for(i in 1:ncol(phatBest)) {
  pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
  perf <- performance(pred, measure = "auc")
  print(paste0("AUC ", names(phat.list)[i], " :: ", perf@y.values[[1]]))
}

```


```{r}
pred = prediction(phatBest[,1], df.test$Retained.in.2012.)
perf = performance(pred, measure = "lift", x.measure = "rpp", lwd=2)
plot(perf, col=1, ylim=c(0,2))
abline(h=1, lty=2)

for(i in 2:ncol(phatBest)) {
   pred = prediction(phatBest[,i], df.test$Retained.in.2012.)
   perf = performance(pred, measure = "lift", x.measure = "rpp")
   plot(perf, add = T, col = i, lwd = 2)
}
legend("topright",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod), lwd=2)


```

```{r}
logit.pf.curve <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,1])

lasso.pf.curve2 <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,2])

logit.pf.curve3 <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,3])

lasso.pf.curve4 <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,4])

lasso.pf.curve5 <- ProfitCurve(60, 0, -40, 0, df.test$Retained.in.2012., phatBest[,5])

```

